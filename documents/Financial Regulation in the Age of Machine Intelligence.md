---
title: Financial Regulation in the Age of Machine Intelligence
notoc: false
---

created: *18 Jan 2019*; modified: *21 Jan 2019*; status: *in-progress*; confidence: *likely*; importance: *8;* by*: Derek Snow*


**Main Points**
 

- Machine learning plays a dual role for regulators; they can leverage machine learning internally to enforce compliance, and machine learning technologies will also be a cause for new or adapted regulations as applied to FIs (Financial Institutions).
  - Regulatory technologists need internal knowledge of AI to further improve compliance process automation.      
  - Regulators will require AI-risk engineers to understand the potential downfalls of AI in FIs.


- Machine executable regulations and AI techniques would allow for automated enforcement, this includes sentiment analysis over policy documentation as well as automated risk assessments, market surveillance and anti-money laundering.
- Regulators should invest in obtaining alternative datasets to stay ahead of institutions attempting to escape compliance.
- Automation succeeds with simultaneous ease-of-use improvements for both employees and clients.
- Regulators should shift focus from a rule-based to data centric approaches for both cost-cutting and accuracy purposes.
- Regulators should understand how adversarial attacks and reverse engineering can undermine future regulatory automation activities.
- AI implemented in a company is nothing more than a function replicator or automaton; to this extent whatever the corporate motive is, the AI motive becomes. Thus, we need to focus at the corporate level to ensure that AI enables a fairer, stable and more inclusive financial system as it risks doing the opposite without supervision. This is nothing new and simply echoes the well-known technology-neutral approach to regulation.
- Automated and customised machine learning models are going to make individually developed models hard to audit, hence the need for regulators to audit activity outputs.
- Know that a large part of future dangers in AI lies with third-party AI and data providers. Although no bad players have been identified the incentive of data abuse remains. 
- Certain types of data and most AI models should be not be made transparent as a way to mitigate adversarial attacks, rent-seeking, contagion, discrimination, blame-shifting and privacy concerns.
- Understand that there would be an increasing tendency for machine learning scapegoating due to the ‘in-auditability’ of many of these models.
- Machine learning customisation and personalisation can lead to harmful outcomes; there are many anecdotal stories of how excessive personalisation can lead to financial or emotional ruin.
- Data alliances can be as effective as mergers and acquisitions; machine learning models are reasonably homogenised; hence data becomes the crucial ingredient; data-alliances will lead exacerbate rent-seeking behaviour.
- The democratisation of AI models and competitive data creates invisible monopolies; companies should focus on sharing only collaborative data.
- Regulators should play the role of data platform intermediary, which would allow regulators to automate compliance using an automated big data approach.
- Together with institutions regulators have to redevelop ways for FIs to share all their non-competitive (collaborative-compliant) information.
- Data partnerships makes it hard to hold parties liable for privacy breaches; machine learning needs a lot of data increasing the concern for data privacy; data has the tendency to accumulate at third party providers who are less incentivised to keep data safe.
- As a result of the monopolistic forces of data you can expect to see a lot more acquisitions and see the growth of multi-facetted collaboration; data competition will force smaller firms to compete by providing new algorithms and products, while traditional players will leverage their data and existing customer base.
- The future success of AI lies in the hands of companies closest to the user-activity interface conditional on the activity being automatable.
- Regulators should pay special attention to the de-anonymisation of data, a lot of anonymised data sets still leaks user information.
- Alternative data sources can lead to certain FIs to use extractive and manipulative business practices.
- Centralised systems combining personal and commercial data are becoming a grave concern to privacy, especially when the users are uniquely identifiable in digital identity systems.
- Data portability does not have a very large impact on competitive dynamics; technology companies mostly compete through product development.
- Even though machine learning decreases overall bias and errors compared to manual systems, new errors and biases unquestionably emerge.
- The main issues of AI systems can be divided into systematic bias and errors as well as progressive and precipitous contagion.
- Bias can occur in the input data, model and metric selection.
- We will likely see employee error diminish and executive error maximise with increased optimisation.
- The first thing to understand is that machine learning is purely backward-looking. This results in inadequate equilibria where state changes are not selected for as a result of being outside the decision boundary and thus left unincorporated in training set.
- A further concern of AI models is post-training drift towards discrimination. As AI systems self-improve and learn, they may acquire new behaviours that have unintended consequences.
- An important point for regulators to understand is that the company with the widest and largest quantity of data, ceteris paribus, would produce the least biased model.
  - For example, to remove a gender bias in a lending model, it is not as easy as removing the gender variable from the dataset. The reason being that the model will find correlations of the remaining variables with the removed variable. For example, height is one of multiple variables that correlate with gender so, by simply removing gender, height would be given more importance in the model and the bias will persist.  Instead you should include as much bias relevant data as possible and then adapt the model to marginally remove these biases.


- Another option is for regulators to, instead of predicting actual biases, to use FIs’ metadata to create bias-proclivity prediction machines. For lending companies, regulators can build up databases that include among other things the ratios between approval and denial rates, the average prices, volume and data on complaints which can be fed into a machine learning model to assess the likelihood of bias without creating hard rules.
- Along with the potential development for special use-cases, it is also important to create awareness and establish strategic partnerships across government agencies.
- Prescriptive regulations are currently limiting the advancement of AI in financial services. The best way to track bias is with comparables, which again comes down to data collection practices.
- A primary purpose for regulators would be to supplant otherwise costly compliance supervision with automated systems using an outcome comparison approach.
- A big issue in AI is the tailored experience. Biases would only be identifiable using comparable data across different FIs. Then it becomes an issue of least and most biased institutions. A further issue here is the potential of collective collusion making this comparable approach useless; this tendency has to be carefully monitored.
- AI Technology is too revolutionary to simply rely on past regulatory actions after supposed ‘paradigm moving’ technologies e.g. Internet. The age of AI calls for scientific reactive regulations rather than costly preventative measures; adaptive sandbox strategies might just be the correct solution; there is a certain beauty in small isolated failures.
- Regulations should be broadly written; otherwise FIs will simply not comply in the new do-first-apologise-later era. The aggregate effect of narrow regulations would simply be to constrain innovation and national competitiveness. In this modern age, companies will start where regulation or the threat of regulation is marginal.
- The ability to use cloud-based services, the ability to use public and private cloud infrastructure, and to use the kinds of data that can be hosted on the cloud, is critical to the development of AI applications. It is essential for cloud technologies to be trusted and audited. It is a technology that is not too hard to audit and supervise.
- Differences in data standards and regulation are apparent across regions, this challenges institutions that operate globally and necessitates international frameworks to manage common systemic issues that would have an impact across jurisdictions.
- Individual countries can test out different regulations; country wide implementation can in itself serve as a pseudo-sandbox for partnering countries.
- Regulators have to also know that it is often easy to effectively translocate AI models developed in more permissive data regimes to less permissive ones.
- Data regulations formulated in the coming years will have long lasting effects on financial markets. In many jurisdictions, data regulations are still being developed. In the coming years, these regulations will solidify, and financial markets will be shaped by those regulations for the foreseeable future.

## Introduction

Some are convinced that in order for regulators to shape future financial regulations they require in-depth knowledge of AI. That is not entirely true. Regulators instead need to collaborate with risk engineers to focus on the contagion, discriminative and systematic errors posed by machine learning activated companies on the market. However, it is important that regulators pick up knowledge in AI for internal purposes. The monitoring and supervision of automated systems have to be dealt with by regulators’ own automated systems. The reason for this is the expected explosion of regulatory requirements due to increased automation. Regulators are expected to progressively develop automated intelligent systems to ease the internal burden that accumulates in the machine intelligence age. For the most part regulators should remain technology-neutral while seeking to understand the behaviour and activity outcomes of machine activated firms rather than audit their algorithms. This article considers the potential of a new form of adapted regulation focusing on data and comparative compliance. By being early in setting up internal AI and data science units, regulators would come to get in-depth first-hand experience to where these technologies tend to fail and to delineate between what is hype and what is possible.
 
First let’s consider the dual role of machine learning for regulators:

- A) Leveraging machine learning internally to enforce compliance.
  - Regulatory Toolkit
  - Adversarial Agents

 

- B) Supervising machine learning active companies.
  - AI Implications 
  - AI Failure Checklist
  - Adaptive AI Regulation

A)

## Regulatory toolkit

There is a growing consensus across the globe that regulators will be enabled to maintain a high standard of compliance and prudential standards through effective AI, machine learning and big data solutions. Now is the time for regulatory organisations to proactively plan to migrate from legacy systems to the latest regulation technology and educate their staff to embrace the power of AI.

How machine learning will become part of the supervision and enforcement toolkit:


  - Data Approach,
  - Machine Executable Format,
  - Automated Enforcement
    - Automated Policy Compliance
    - Automated Risk,
    - Automated Market Surveillance,
    - Automated AML,
    - Investigative Automation,
  - Ease of Use


### Data Approach



Regulators should focus on moving away from rules-based towards data centric approaches in conducting most of their activities. In a recent speech, FCA[[1]](#) representatives indicated that they are moving away from a rules-based, prescriptive approach of regulation to a more data-driven, predictive place where regulators use data to help them objectively assess the inherent risk posed by FIs.
Regulators should invest in obtaining alternative datasets. This can be done by purchasing datasets or leveraging easy to use technologies like web scraping to source data that can aid decision making.  Real estate activity like volume and prices can for example be gathered from a range of real estate websites and used to guide interest rate and LVR policies.

### Machine Executable Format

Regulators should actively partake in converting their rules into unambiguous machine-executable format. Doing this would facilitate the investigation of firm’s compliance as their actions become machine assessable. Systems should be set up to flag financial institutions compliance directly by leveraging AI/ML solutions. The first path would be for machines to analyse regulatory filings to quickly scan for inconsistent submissions. Regulators can both make use of non-cognitive robotic process automation tools and cognitive activated systems.

### Automated Enforcement

Regulators should decide among competing policies based on how automatable their enforcement are. 

#### Automated Policy Compliance:

The recent (25/05/2018) GDPR regulation require firms to clearly explain how users’ data is collected and with whom it is shared. It further requires of firms to explicitly consent to retain and process customers’ data. Financial institutions’ privacy policies can be compared against EU GDPR policies using advanced NLP technologies.

1. Identifying 3rd parties with which user’s personal data is shared using entity recognition.
2. Assessing the readability and complexity of the policy.
3. Getting a sense for the sentiment of the policy document (giving special attention to coercion)
4. Identifying whether users' consent is implicit or indeed explicit (as mandated by GDPR)
5. Privacy policies can then be rated and those who fall below a threshold can be flagged and sent to supervisors and agents for further investigation.

Policy departments can create space for machine adjustable variables to advise decision making. Machine learning models can act quickly by scouring past data. Monetary policy looks like one of the areas most ripe for change. Central banks in Russia[[2]](#) and China[[3]](#) have both invested in these technologies to improve their decision-making quality. The Russian Central Bank created a new economic indicator to assess national economic activity using online and other sources, while China has looked at machine learning as a means to identify and defend against market and sector financial risks.  If this trend persists, central banks are soon to use predictive machine learning models to identify changes in short, medium- and long-term economic trends. The folks at the US Federal Reserve commented that machine learning lets the available data speak for itself, potentially revealing important relationships that have not yet been identified by theorists.
In the future we can expect central banks to allow for a component of their interest rate to be managed and executed by automated AI systems. Real-time transaction level data can be fed in from central banks’ and relevant governments’ rich databases that reports on consumer behaviour, unemployment rates, government spending and production levels. This is especially true for central banks that do simple inflation rate targeting such as the USA, NZ, UK and Canada. The intelligent use of new and old data points can help to provide for multiple possible solutions using a scenario-based model. The issues can be dealt with in real time; these dynamic policies can lead to significant efficiency gains. Imagine a policy rate that fluctuates on a daily basis to assist economic growth and other purposes. Naturally there are disadvantages to automating and giving up direct control of these important economic levers, but this topic will be left for future discussions.

#### Automated Risk:

Scott Bauguess, Chief Economist of the SEC, gave a speech on the risk of AI and Machine Learning. He stated that this technology will no doubt make the risk assessment process more efficient and effective, but is not likely to replace human judgment in regulation of financial markets. Like any automated process, once the correct data pipeline has been established the rest follows quite easily. Once data is injected in the machine learning models they can, for example, speedily assess financial institutions’ (especially banks) metrics and features to assess the level of risk and whether or not they comply with liquidity and capital requirements. Such systems can be used to study and flag institutions with anomalous looking indicators without much human intervention.

#### Automated Market Surveillance:

It is relatively undemanding to use machine learning solutions to identify and monitor market misconducts. It once more important to use a large datasets of diverse data. Automated market surveillance can be quite comprehensive and look at areas of market manipulation such as pump-and-dump schemes, insider trading and advisor misconduct, among other things. The data sources for such models are endless. A good and simple start would be trading firm and exchange data. Alternative data are becoming increasingly available; unstructured data such as conference calls, are easily convertible into a machine readable format. In saying this, regulators should remain aware of the quality of the data and try to follow strict standardisation protocols.
Using ML model goes beyond mere identification of market manipulation practices. Instead of hardcoding filters, cluster analyses may be used to flag anomalous behaviours. It is possible that some manipulative activities can span multiple types of known activities (like spoofing and layering) so as to, in aggregate, stay under the radar. Without commenting on the importance of this form of regulation, FINRA[[4]](#) established a project to investigate cross-asset manipulation by investigating investors who holds underlying options positions in an attempt to move the market.
Further tools can be developed to spot anomalies and outliers from structured and unstructured data. The DERA division of the SEC looked at patterns of regulatory filings of investments advisors and unsupervised topic modelling and tonality analysis ML techniques to uncover strange behaviour and idiosyncratic risk[[5]](#).

#### Automated AML/CFT:

Investigating AML and fraud is extremely costly and time-consuming. There are an extremely large amount of suspicious activity reports filed by financial institutions. Institutions over-report ‘disclosures’ to remain on the safe side, this leads to a large amount of false positives for the regulators to deal with. In such scenarios you can create machine learning models to fairly confidently predict the likelihood of clean transactions, which in turn allows you to shortlist truly suspicious transactions.
Network analysis in combination with ML tools can further help to assess fraudulent and AML concerns. When we simply focus on individuals, we lose touch of transaction flow. By taking a step back and looking at entre transaction networks, group complicity can be observed. An example would be to compare bank transaction with regulatory sanction lists and unstructured data to uncover relationships and detect money laundering patterns. The AUSTRAC[[6]](#) agency have used such steps in identifying previously undiscovered money laundering networks.

#### Investigative Automation:

One off machine-learning models can also be used to investigate individually concerning matters. The ASIC[[7]](#) has for example used machine learning and web scraping technologies to unearth potentially misleading conduct on accountants websites for self-managed superannuation fund activities.

### Ease of Use

Machine learning and good UX design can be used to significantly improve overall user satisfaction. The FCA of the UK collaborated with Corlytics to create an intelligent, easily searchable regulatory handbook. Further improvements can be made with their client and/or user complaints interface. Regulators should by all means possible foster compliance by facilitating firm's and users' access to regulatory information. AI technologies like chatbots will be well suited for this purpose; elsewhere FINRA[[8]](#) has noted that chatbots has as a potential use case in broker-dealer relationships. These chatbots can further be utilised to conduct sentiment analysis of customer complaints and topic modelling to flag complaints according to specific categories e.g. unjust discrimination by a financial institution. 


## Adversarial Agents:

Generative adversarial networks (GANs) are two machine learning models pitted against each other (hence the word “adversarial”). In simple terms GANs attempt to simulate reality. An analogy can be drawn between a hacking defence system that has to stay one step in front of hackers that are simulating honest intent. A notable and recent use of GANs is Christie’s auction of an AI generated painting to simulate art between the 14th and 20th century[[9]](#). GANs are also the technology behind voice, video and image replication.

Researchers have shown how GANs can be used to fool a Google AI model into believing that a helicopter is a rifle[[10]](#). In the future a publicly known regulatory AI model can be fooled into believing that non-compliance is compliance from a perfectly machine massaged data from an adversarial attack. The US treasury have noted that while machine learning is currently being used to enhance fraud protection, AI could potentially be used to circumvent these same fraud detection capabilities.

Therefore, in the future we can expect a ‘battle’ between automated systems. Adversarial agents are going to attempt this in two ways, first they will try and simulate automated regulatory machine learning systems using data available to them. When data is not available to them, they can seek to reverse engineer the automated regulatory system by creating ‘fraudulently’ flagged instances and getting feedback. In this way they can discover model decision boundaries and probability thresholds. 

One might say, yes okay the models can be regulated, but the adversarial agent does not have access to the regulators data. That is true, but the agent does not need the exact data. Instead the agent simply needs the public record of all companies that have and have not been flagged for a certain regulatory issue and then they can retrofit *any* data they might have that covers these companies. If the model achieves great success on an out of sample dataset then this model can essentially be used to decipher correlated features with the proprietary regulatory data. 

For example, Moody’s and S&P’s rating agencies can be reasonably successfully reverse engineer using publicly available data and machine learning models. Further it has been shown that machine learning models have been successful at replicating bank analysts and credit rating agencies. See the example of random forest by Moody’s. Wherever regulation relies on the input of data, GANs can be used to ‘trick’ the system into believing that the regulated are complying with said regulations. In its current form GANs are however somewhat reliant on datasets being large.

Here is an explanation of a possible future scenario. In a standard machine learning model managerial or financial accounting data can be fed into the model with data labelled as fraudulent and non-fraudulent. Future observations can then be fed into this system to predict the likelihood of fraud. With a GAN model, instead of company accountants playing around with financial data to ensure that the predicted level of fraud remains below regulated thresholds, machine learning models are built to ensure that the largest amount of fraud can be committed without being flagged. Thus, GAN and GAN-like models would become a tool to bypass regulatory flagging.

What can regulators do to fight off competing models? By far the biggest pressure would come from third party services who would have accumulated enough data to execute these types of 'attacks'. Regulators would have to stay one step ahead of these companies, both in the quantity of data and the quality of their models. To fight off these attacks its first essential for regulators to keep their models hidden, as it is extremely easy to just copy a whole model (intelligent system) on a pen drive and distribute it to the highest bidder, and it is not exclusionary, in fact this leaked information can be widely shared without FIs being caught out. However, there is a method to measure the number or intensity of adversarial attacks. Similar to the use of a funnel plot used in research to identify publication bias, regulators can investigate the clustering of companies who pass regulatory inspection just above the selection threshold.

Regulators can further prioritise the use of privately available data as well as incorporate a wide range of datasets overall to fight off some competing models. And in the same breath, regulators should refrain from making their data public. Further, regulators can dupe GAN’s by simulating and distributing false information about non-existent companies and regulatory breaches. However, this should be a last resort. Regulators also have to pay special attention to third party companies that aggregates individual FI data for the purpose of providing some service. They, in the process of providing the service can potentially use the data to undermine the regulator. 
Another issue is the generation of realistic looking data. When FIs are asked to present a copy of their data, they can simply provide simulated data. With GANs it is easier than ever to provide realist looking data that would go completely unnoticed by human auditors.

In the future it would not be enough to simply use models to flag (such as by the Monetary Authority of Singapore) suspicious transactions. It does provide a good line of first defence, but regulators would also have to investigate those who on paper seems to pass the compliance test but are in fact duping the system. They can have both a proactive strategy by trying to detect adversarial attacks, or they can investigate systematic biases in submissions and then randomly investigate those clustering around the selection thresholds after submission. The existence of GANs means that the future of regulation is indeed one where models will be fooled to the extent where humans won’t. Hence good old randomised spot checks are not disappearing anytime soon.


![](https://d2mxuefqeaa7sj.cloudfront.net/s_05843F161ED5ED846961C9ECBA94F4D93633DC01A0245E78F076D04C4D4874A7_1549069016687_image.png)




[Link](https://arxiv.org/pdf/1811.11553.pdf)


**B)**

## AI Implications

- Company
- Models
  - Audit
  - Automation
  - Transparency
- Corporate Scapegoating
- Algorithms, Third Parties and Liabilities
- Customisation
- The Changing Face of M&A
- Monopolistic Force
- Data Competitively Defined
- Regulator as Data Platform Intermediary
- Data
  - Data Sharing
  - Data Privacy
  - Crafty Alternative Data
  - Digital Identity Systems
  - Data Portability

 

### Company

In the future smart (AI, ML) and hard rules would drive a lot of a company’s decision making. To the point, it is not hard to imagine a large portion of company processes being driven by AI (colloquially dubbed a self-driving business). For this and other reasons, it is good to use the analogy of AI as a company to understand the coming dangers of AI. Why? Because we have been facing the same issues from companies for millennia. At its core a company, like an automated model, is a collection of supervised agent/s incentivised to pursue a goal/s. Therefore, like a company, It is not worth regulating away AI as it is a generator of wealth. However, in that same way that companies are known to act in the benefit of select parties at the expense of others in the pursuit of shareholder wealth, ethical dilemmas do arise in AI systems. At first, the company draws a circle around the AI system and then the AI becomes the circle.  In this circle the AI system has the same objectives of the company, which for most is growing shareholder wealth. This objective function is programmed into the core of the AI system.

Today, AI/ML based solutions are being leveraged by financial institutions (FIs) for a variety of use cases – such as customer segmentation for improved marketing, cross- and up-selling, campaign management, client-facing chatbots, creditworthiness evaluation & credit score prediction, augmented products recommendations & personalized financial advice, investment & portfolio management, algorithmic trading, trade strategizing & execution, dynamic portfolio rebalancing, capital optimization. The real question we are facing is how and why we should regulate companies that are pursuing these technologies differently to companies that don’t. The answer is somewhat dubious, but it mostly has to do with the fear of losing visibility and the repercussions that arise from it, like blame shifting. As it is, there is no reason to not to, as has always been done, hold the user of the technology in charge of any purported injustices and damage inflicted.


### Models

#### Audit:

Companies will have two types of AI systems. First those provided by third parties and second internally developed systems. For regulators, the centralisation of AI can be a trend that will ease the efforts of direct audits. If the major AI model implementation are being delivered by only a few third-party service providers, then regulators might in fact feel the need to thoroughly investigate and tease out the underlying models of these providers to investigate the potential for systematic bias and error and investigate the potential for progressive and precipitous contagion.

However, this would be in vain; models are slowly developing a mind of their own, technologies like AutoKeras and Google’s AdaNet are learning to automatically create ad hoc context specific neural networks. Hence the inner workings of each new generation of an automated model has to be investigated and not the infrastructure of the automated system. The reason being that biases and errors are only directly visible from the generated model. Even if the above was not true, it is extremely unlikely that third party service providers will grab a large portion of the market. The reason for this lies in the democratisation of AI. The largest companies have for competitive reasons made their machine learning libraries open to the world to use. Hence it has become increasingly cheap and popular to implement your own model based on their infrastructure and programming paradigms. Without centralisation, auditing all these models would be a failed pursuit.

Further even if regulators seek to audit these models, their lack of interpretability poses further issues. As it stands there are thousands of models in action across industries and they can differ greatly across and within industries.  The complexity of AI models raises challenges for transparency and auditing of such models, which undermines traditional regulatory frameworks that rely on an expectation of transparency. For example, the Fair Credit Reporting Act requires that companies notify a consumer if consumer report information is used to deny credit. It may be difficult for firms using AI to make credit decisions to provide notifications and rationales for such decisions. As a result, auditing individual models would be infeasible and close to impossible.

#### Automation:

The Automation of ML from technologies like AutoSklearn, AutoKeras and Google’s AdaNet means that soon everyday computer-users without programming expertise can start to implement models. This poses a significant the threat for regulators in the sense that every-day users would not be able to unravel the workings of a model to understand its potential for being biased and error prone. These models are generally created to perform extremely well on technical tasks, where a decrease in the error rate is always a good thing and is seldom at the expense of something else. For example, these models can be used to identify movie characters and authors, simulate painting styles and predict tomorrow's weather, where better performance leads to a net-positive outcome. However, once these models carry over in the business world and are instructed to e.g. maximise profit, customers and revenue, then it will always be at the expense of something else. Thus, in the business world there should be a big push to ‘fair’ automated models and regulators should lead this push and advice against the implementation of bias-blind automated models.

#### Transparency:

Although it would, prima facie, be great for these companies to make their models transparent and publicly available, this opens up the potential for using GAN models to exploit models for financial gain (See the Simulation section). Hence companies would not be willing to do this. Making the model openly available can lead to customers bending the truth in their applications to ensure that they do indeed meet the minimum thresholds of for example a loan. This misuse would be noticeable in the clustering that occurs just above the minimum thresholds for loan acceptance. Further, companies should also be weary of client credentials before a query-feedback process. If users are able to query and get feedback without creating an account, then the machine learning model (or decision model) used can effectively be reverse-engineered. This has become very in vogue and is in fact not too hard to do[[11]](#).  Lastly companies might not want to open source their models to maintain their competitive advantage; this is however a secondary issue to the above more overlooked issues. In essence, the evidence points away from model visibility and open source development strategies. 

#### Corporate Scapegoating:

The main issue with black-box machine learning is not that they are hard to decipher and interpret. We don’t say humans are hard to decipher -- unless they are unwilling to respond or respond randomly to our questions. Black-box models can be understood by tracing the inputs you used to the outputs generated.  We can seek to understand black-box models by altering the inputs and investigating the effect on the output. The real issue with black-box models is that companies and corporates would be able to justify their decision making under the guise of it being a ‘black-box’ model.

The next disaster is bound to be blamed on AI models, not because they are at fault, but because they are a very easy target of blame. Unlike their aviation counterparts, post-mortems on these models are non-existent and they become excellent scapegoats to persecute. This will lead to more unnecessary regulatory burdens. Most models are in all truth, not black-box and are more likely to be white to grey-box but are unfortunately put in the too-hard-to-understand box as a result of user-group incentives.

#### Algorithms, Third Parties and Liabilities:

Third-party data providers will at some point become critical in developing AI systems due to the data-hungry models underpinning improvements in performance. Companies will, as a result, increasingly rely on previously unregulated stakeholders. The incentives to join a third-party provider would be enormous. If you can plug into an existing data lake and model zoo you will have an immediate benefit, and all you have to do is give up your user data and a few pence. You will at the point of joining, immediately outperform companies who simply rely on self-generated data and models. These third-party data providers, being the entities with the all lucrative data, should become the prime targets for future regulation.

Third party data aggregators open up many points of concern that has to be addressed to avoid market failure. These parties should not be allowed to carry any non-encrypted and non-anonymised data. This is for two main reasons.


- First the data can be used to dupe regulators with adversarial attacks.
- Second it draws a safety-blanket across multiple companies, effectively creating a group of rent seekers.
- Third, the bigger the group, the less competitive the market becomes and the more likely the discriminated parties are to be fully neglected by the market. 
- Fourth, because they carry so much data, a single breach can have disastrous implications.
- Lastly, FI’s will use third-parties' data/models to shift blame.

Competitive third-party data providers should be squashed, unless the benefit of additional data can be immediately transferred to the improvement of data-reliant models after which the data gets deleted. The data can theoretically be used by third party providers to only improve models and not to carry static data so as to avoid being a harbinger for privacy issues. Thus, there should be a big push away from data-alliances towards model-alliances. Model-alliances still doesn’t rid us of the monopolistic and blame-shifting issues, but it is a first step in the right direction to address privacy concerns. Simple techniques like encrypting and deleting data can to a degree be audited to ensure compliance. 

The next question we have to ask is who is to be held liable. Institutions are going to become increasingly locked into mutualised services build on collective data. These companies would not be able to compete without the efficiency of collective services. Individual institutions would need significant innovations to develop algorithms that overcome their comparative deficit. In machine learning it is possible to simply incorporate one model within another, there can in fact be multiple layers of models, this is known as stacking. Thus, in the future AI will inform AI’s making it increasingly difficult to identify the source or entity that has overstepped. Especially if each AI is provided by different service providers. Unlike a automobile where you can point to the component supplier that delivered faulty break-pads, it would be close to impossible with stacked machine learning models. Hence regulators might have to simply hold the final provider liable for damages inflicted.

#### Customisation and Personalisation:

Machine learning will push the levels of customer customisation. Marcos de Prado from Connell University and AQR Capital Management says the future investment funds will be set up as a one fund per client model to satisfy the individual needs of customers. In the past, personal financial management apps were restricted to describing a customer’s financial situation; they were unable to provide actionable insights and recommendations.

The next generation of these services (e.g. Clarity Money, MoneyLion and others) are using AI to offer mass advice and customization to help improve customers’ financial positions (e.g. refinance a loan, consolidate credit card debt or cancel certain recurring payments); similar tools are being built for corporate clients (e.g., institutional investor dashboards.) Multi-provider platforms are increasingly looking to empower their offerings through personalized recommendations for which products and features are best suited to customers. Credit Karma, which has found success as a lead generator for loans, raised $500 million in March 2018 to build financial adviser tools and extend their control of customer experiences.

There are a plethora of issues that should concern companies about customisation. One telling example is the case where companies like Google and Facebook kept trying to advertise infant products to mothers who have miscarried. These models incorporate individual’s search record and a sudden change in circumstance can lead to this sort of advertising being very hurtful[[12]](#). The same can be said for financial ads targeting. It is possible that although your search results are of such that you seem to have a lot of wealth you could have hit recent economic disaster and the momentum of your previous searches persist and in the worst case such advertising can send you down a spiral of debt.

#### The Changing Face of M&A:

Modern data alliances are as good as mergers. Data is the secret sauce to squeeze future efficiency and profit gains. Machine learning models are reasonably homogenised; hence data becomes the crucial ingredient. If data is shared it effectively leads to larger companies. Depending on the setup, these data alliances need not be made public. Hence it can go under the radar compared to legal mergers. This will and is leading to a phenomenon that can be dubbed *monopoly by data*.

#### Monopolistic Force:

*Yeah, OpenAI was about the democratization of AI power. So that’s why OpenAI was created as a non-profit foundation, to ensure that AI power ... or to reduce the probability that AI power would be monopolized* – *Elon Musk, OpenAI Founder.*

The democratisation of AI as a means to remove the monopolistic forces of AI is a useless activity, modern monopolies are mostly, if not fully, driven by data. Data is the fuel and machine learning models are the furnace. You can democratise the furnace, but without the fuel its useless. On a further point, as great as the recent advances in *open data* has been to allow some smaller firms to compete against the larger powers, open data protocols have still been mostly to the benefit of large strategic players, the reason being that the open data initiatives are mostly sourced from public or non-profit entities instead of being proprietary data being scalped from large rent seeking companies. Data has become the invisible rent-seeking tool hiding in the cloud’s pearly gates of heaven strictly guarded by Saint *Don’t Be Evil* and friends.  

Further if you democratise the data and the models then it would further advance monopolies as they are most competitively positioned to swiftly incorporate additional data into their established models and to use it to capitalise on existing clients and networks. Open data as done currently is a net-detriment to small players, however large movements in corporate open data can be beneficial. The large players are those that can stand around the fire while stoking it with fuel. Without the network being democratised (at this point really it is communised) or mass corporate open data schemes -- which has many other downsides by the way -- there would be no benefit for wide-spread model and data democratisation. Moreover, AI is naturally monopolistic, it is poised to create data-driven connections across industries and borders to deliver value in differentiated ways but is extremely susceptible to creating excessive concentrations of market power that further drives income inequality. AI monopolies will start to charge more money because their large body of data allows for mass-scale behavioural analysis leading to magical customisation and ease of use. Excessive data ownership will unwittingly trap consumers like a moth in a bath. The large body of users enlarges the risk of contagion from massive data breaches which can lead to mass-manipulation campaigns. We have seen some signs of this with Cambridge Analytica and their pysops campaigns[[13]](#). Further any form of error and discrimination will be widespread and can inflict a large amount of damage in a short span of time.

As a result of these monopolistic forces of data you can expect to see a lot more acquisitions and the growth of multi-facetted collaboration. Data competition will force smaller firms to compete by providing new algorithms and traditional players will leverage existing product expertise and their customer base. The future success of AI lies in the hands of companies closest to the user-activity interface conditional on the activity being automatable. Marc Andreessen says *software is eating the world*, this will not remain the status quo. Companies like Uber will be outcompeted by companies like Tesla once software becomes ubiquitous. Tesla already has a customer base for a physical product (an automated car), to reproduce Uber, they simply have to slap navigation and payment software into an app given that they have already collected an immense amount of data. App creation has become exceptionally easy with API integrations.  The going rate to professionally reproduce the Uber app is in the low thousand. Therefore, it is not software that would eat the world, its hardware connection and data collection companies that are *eating the world*. And among two companies, those who can do both hardware integration and data collection is bound to succeed. We can see this playout with the recent acceleration in hardware releases from both Google (Nest, Smartphones, Laptops) and Amazon (Kindle, Servers, Alexa) and hence the reason why Uber has started collecting data with their own self driving system. Physically-integrative data-collecting companies will *eat the world.*

In the United States, data-sharing alliances are more ad hoc than mandated, with individual banks building bilateral relationships with data aggregators. Regulators have not signalled an intention to implement frameworks similar to the UK and the EU.27 However, the US Congress has been listening to testimony from large technology companies such as Facebook, Google and Twitter on the topic of privacy and data security, which could lead to the emergence of new rules.  From now on, if you would like to know where the power lies, follow the data.

#### Data Competitively Defined:

There is a distinction between competitive and collaborative data. Competitive data leads to increases in market failures as a result of its market concentrating effects. An example would be to share anonymised customer characteristics and default history. Collaborative data can be either Compliant or Fraternal data.   With Compliant data, the open sharing leads to decreases in market failure and are mandated by the regulator. An example would be a timestamped list of potentially fraudulent transactions. Fraternal data is not regulated data but nonetheless the data leads to the fairer treatment of customers across different sectors. An example would be a dynamic list of clients who are in-fact, contrary to public credit scores, stand-up citizens. This type of data would normally be shared across sectors instead of within sectors otherwise it might become competitive. It is therefore not beneficial to share all data within sectors and some of the data that should not be shared within sectors should be shared across sectors.

#### Regulator as Data Platform Intermediary:

Unlike private institutions, regulators can’t seek to understand the inner workings of AI models. Regulators should redevelop the way it assesses compliance. Regulators would have to become platform and data support administrators. This would allow regulator to automate compliance using an automated big data approach. Together with institutions they have to redevelop ways for companies to share all their non-competitive (collaborative-compliant) information. These tasks might involve many intermediary steps like the anonymisation of data. Regulators have an amazing chance to take on this intermediary role as a result of institutions who would like to combine, they data to reduce compliance costs of individualised regulatory interactions. Regulators can then set up tools to auto-investigate compliance on the large data lake.

Corporations are already motivated to create strategic data alliances by sharing collaborative data.  They must find a way to balance their competitive impulses against collaborative opportunities. Addressing the large regulatory issues would require public-private engagement. This engagement should preferably take place with bodies that are somewhat removed and independent from the main regulatory body. Companies want to form strategic alliances to collaborate on issues where the current inhouse processes are rarely a key differentiating capability. Which in turn helps them to relieve costs through economies of scale and additionally allow them to improve the function due to not just shared data but shares expertise.

Further it is known that a lot of these processes are sector agnostic. For example, the requirement of producing financial statements and engaging in annual audits. AI presents a strong mechanism to collaborate as the value of shared datasets is tremendous AI benefits from scale in data to deliver performance that is greater than the sum of its individual parts. Processes that often involve large datasets and flows of data are prime targets for AI implementations.  AI can recognize patterns and develop insights on threats that cross institutional boundaries. As institutions move to create common utilities, new frameworks that address talent, governance and technology standards should emerge.

“Early-stage collective utilities are emerging, backed by key service providers. Collective institutions such as SWIFT and EarlyWarning have started developing service offerings that will leverage AI and the collective power of data to address some of the biggest threats. SWIFT is launching a new intelligent in-network solution for fraud control that combines real-time monitoring, and alerting and blocking of sent payments, with daily reporting. The solution gets “smarter” over time and is part of SWIFT’s commitment to develop leading-edge technology solutions, including AI and ML, to help its customers address their regulatory, fraud prevention and risk mitigation requirements. EarlyWarning is a fraud and risk-management technology company, started by a collective of the largest US banks, that employs AI. Further companies such as ComplyAdvantage and Shift Technology have demonstrated significant benefits in using AI-based algorithms to monitor transactions. ComplyAdvantage claims to have achieved an 84% reduction in false positive alerts for AML risk data, while Shift Technology is helping insurers fight claims fraud using AI.”[[14]](#)

The safety of the financial system will be radically improved If collective solutions succeed, real-time scanning using full market data has the potential to dramatically increase institutions’ ability to react proactively to threats and catch malicious activities with improved accuracy. Collective endeavours will help banks and institutions keep themselves in check. These institutions can be semi-autonomous, but the burden has to lie on these institutions to cooperate. You can create forced cooperation, but it is still important to find the right ownership framework for collective utilities to ensure their interests are aligned with their stakeholders and to design a liability framework for errors and compliance failures e.g. liability shared between utilities or should individual fault be identified and the liability isolated. It is important that system is so designed that collective accountability is fostered rather than the offloading of risk to this central utility.


### Data

#### Data Sharing:

Institutions are hesitant to use third-party services for AI systems because regulators will likely hold the primary institutions responsible if there are damages to recover. Third parties should never be held responsible, the third parties are where a lot of the innovation will happen as long as they hold to some of the principles as outlined in the third-party section. 

Algorithm audits should entirely be scrapped; it’s next to impossible for regulators to successfully grasp the underlying drivers of all the models currently in use by FIs. Further it is unlikely that the audited model would be the model used in practice. These models are constantly changing and modularise so easily swappable and easy to hide.

This is false, because you only give over the minimum requirements. Second, to get customers to switch you first need to build a great customised product which would be the norm in the future and for that you need customer data. So instead they would have to create models that in fact pay for customer data. Like $5 if you hand over your banking data. Or hand over your data and get $5 in your first deposit account.

The UK has been one of the first jurisdictions to adopt open banking as a mandate across financial services. This push started in 2016, with a report by the Competition and Markets Authority that found “older and larger banks do not have to compete hard enough for customers’ business, and smaller and newer banks find it difficult to grow”. Across different parts of the world, governments are considering radical changes to their data-openness regimes. Australia, Singapore, Canada and Iran, among others, are actively considering some form of the open banking regulatory model, often mirroring the steps taken by the EU and the UK. These data regulations often extend beyond financial services and affect many different industries collectively. For example, the *G20’s Anti-Corruption Working Group has identified cross-sector open data as a priority to advance public-sector transparency and integrity*. – This to me is a great idea.

The marginal benefit for an individual benefit to share their data to a third-party provider is high in the short run. However, in the long run the third party provider obtains the competitive advantage as a result of the data usage. Hence the *give some data for a service* model will likely succeed game theoretically, but it is to the disadvantage of the market in a multi-step long term game. Incumbents must play an active role in shaping data regulation if they are to remain competitive. Wide-reaching, cross-sector data regulations affecting entire economies will determine whether incumbents can access the necessary external data to continue to own the customer experience. It is expected that incumbents will become louder as time progresses. If large financial institutions maintain control over data-sharing (e.g. over the terms of how they share customer data), they may prefer to strike bilateral partnerships with tech companies to access third-party data and AI capabilities. Proliferation of the flow of data (normally APIs) between technology companies (both public and private) have allowed for the successful formation of some incumbent institutions (e.g. WeChat and Alipay). However, some of these API services are still publicly undisclosed or expensive and as a result can inhibit innovation and growth.

#### Data Privacy:

The volume of data required to effectively develop AI raises data privacy concerns as consumer data is increasingly shared without informed consent. Further, AI’s ability to analyse data and identify users may in fact increase the sensitivity of data that was previously considered sufficiently anonymous. Data privacy, security privacy and data-protection regulations (e.g. GDPR) are placing new limitations and requirements on the collection, transmission and storage of personal data Impact on competitive dynamics ‒ Data partnerships become increasingly difficult to manage as parties are held to stricter requirements ‒ Consumers gain increasing control over their data, including control over who can access that data and “the right to be forgotten”.

A further issue is the use of machine learning models to deanonymize users and data. The best way to understand the issue that currently circulating anonymised data represents is to draw an analogy from crime scene investigation. In the past no one was afraid to leave their DNA at a crime scene, however eventually after the development of DNA-kits a lot of offenders have been deanonymized. In the future the lax way data is treated is going to be to the detriment of privacy. Multiple methods are already in development to deanonymize data[[15]](#).

#### Crafty Alternative Data:

Alternative data can be intrusive and simultaneously legal. Data provider *Return Path* specializes in volunteered personal email data covering approximately 70% of the worldwide total email accounts. By collating this with purchase email receipt data for around 5000 retailers, it offers analytics around purchase behaviour and consumer preferences. Similar transaction data is also available from *Slice Intelligence* and *Superfly Insights*. At this point we have to ask, whether this circulation is in fact to the benefit of the consumer or whether it just provides extractive and manipulative power to retailers and hedge funds.

A set of firms have released apps to help consumers manage their finances – such apps track spending patterns and advise their clients. These apps typically gain access to bank, investment, and retirement accounts, loan and insurance details, bills and rewards data and even payment transactions. Such data is then anonymized, aggregated and then sold to third parties. Firms in this category prefer to remain out of the media spotlight. In 2015, *Envestment Yodlee* (partnering with 12 of the 20 largest US banks and tracking around 6 million users) published a rebuttal seeking to refute a Wall Street Journal article (titled “Firm tracks cards, sells data”) that claimed that “the company sells some of the data it gathers from credit- and debit-card transactions to investors and research firms, which mine the information for clues about trends that can move stock prices” )

Companies *Second Measure* and *Earnest* report aggregated and anonymized data on consumer transactions (e.g. see WSJ article; the same article also discusses a potential increase in volatility of retail stocks due to dependence on credit-card data). Second Measure also claims that its Big Data analysis allows it to show how many subscribers Netflix has or how Uber is doing relative to Lyft. Consumer transaction data is also available from data aggregators including *Eagle Alpha*, *Quandl* and *1010 Data*. In recent months we have seen large banks become aware of the value of their “data assets” and hence one can expect them to try to sell this data directly to hedge funds bypassing third-party aggregators. Data itself as well as data-partnership are extremely easy to hide from regulators, this will pose significant compliance issues in the future.

Following is an example of how hedge funds might use consumer data. On Dec 2, 2016 *Advan Research (3 Billion Transaction Data Points)* noted QTD traffic at Lululemon (LULU) locations were up 7%. This motivated them to predict LULU sales to beat the street consensus; a claim that was validated on Dec 7th, when the firm announced sales results. This sales surprise boosted the stock from $58 to $67 within a day. As a result of this data, hedge funds with a lot of data can make more data while small players are left out of the equation. Again, displaying the monopolistic power of data. Data clearly exhibits the Matthew effect of accumulated advantage.

#### Digital identity systems:

“Digital identity systems will be critical to managing personal data flows as consumers gain increased control over how their data is used, they will need a consolidated point of control to easily manage consent and authorization; this is likely to be a digital ID system.”[[16]](#) Some governments have for example started building up systems to connect strands of data with a digital identity, most notably China. However, many other countries partake in similar activities, however these countries do not, for what we know, connect citizenry with commercial data. For example, the RealMe service in New Zealand absorbs all your citizenry data and this centralised system allows you to easily complete multiple tasks after identity and address verification[[17]](#). These activities include the renewal of your passport, tax filings, visa applications, council services, company registration, police vetting and other services. So, you are giving up your data and identity for functionality -- a common trend seen by many a provider in this modern age.
In an attempt to fight for user privacy, it is essential that regulators ensure that user data, especially those attached to digital identities, do not become to centralised because of precipitous contagion risk that data breaches pose.  It is important that data be held by various independent authorities and that they are never combined. We need smaller silos of independent data lakes, so that if the one is poisoned the damage can be controlled. Data tends to corrupt and absolute data corrupts absolutely (a slight altering of Lord Acton’s adage).

#### Data Portability:

Data portability and open banking Regulations in Europe (e.g. PSD2, UK Open Banking) require that incumbent institutions share customers’ financial data with third parties (at the request of the customer.) Impact on competitive dynamics − Large technology firms can access financial data and use it alongside a wealth of other personal data, giving them a head start in developing new AI applications for customers’ finances − Financial institutions do not have the reciprocal ability to access non-financial data from third parties (e.g. technology companies).

If personal financial data became hyper portable, third parties would be able to access incumbent-held customer data and build platforms, and customer data would theoretically cease to be the main source of competitive differentiation. However, this is slightly overhyped. Customer data that flows too slowly into third-party data warehouses would not lead to a competitive advantage, in this case, they would likely compete on product innovation. In the long run more expansive data sharing might indeed erode incumbents’ data advantage if it includes the sharing of big datasets as opposed to client by client access. 

## AI Failure Checklist

How do we ensure that algorithm driven decision-making can be trusted and held accountable? There will be a large reduction in conduct risk as sales activities will be performed through self-driving agents as opposed to sales staff. However, when misconduct does occur it will be more insidious and only visible through machine actions and behaviour changes. We will likely see employee error diminishes and executive error maximise with increased optimisation. When machine misconduct does occur, it will be on a much larger scale due to the connectivity of AI.
So yes, future automation does means fewer conflicts of interest and better performance at a lower cost, but it is traded off with contagion risk is not managed well. AI does create new opportunities to more efficiently and effectively combat bad actors through collective action but opens up the industry to broader risks of contagion as AI demands an increasing interconnectedness across domestic and cross-border systems. Even though the decrease in overall bias and errors, new errors and biases will unquestionably pop up. And the next section deals with this.

Following is the four forms of market failures you can expect to be induced by the use of AI and machine learning systems.


1. Systematic Bias and Errors
2. Progressive and Precipitous Contagion

### Systematic Bias and Errors (SBE)

The first thing to understand is that machine learning is purely backward looking. This unfortunately results in inadequate equilibria as a result of changes in state not being incorporated in training data. This persistence of errors and biases becomes a crucial issue. Life is dynamic and individual humans state change often. Machine learning is very bad with the the treatment of newly appearing groups. As an example, after the women suffrage movement an AI model might still be suspicious of lending out money to women even after they earn a comparable level of money to their male counterparts. The model simply persists old stereotypes as a result of training on historical data. Thus, without introducing this new segment manually into the training data set the model would never seek and select for women due to algorithmic boundaries excluding such selection. Hence to combat such unfortunate consequences, we would hope that competitive firms don’t lose their intuitive grasp of economic sense making and employ humans or altered machine learning models that can correct for this current error in algorithmic decision making. Therefore, humans do have a role to play, until we have models that can do real time causal analysis to swiftly identify changes in the environment. A decision-making process informed by models may be unable to provide explanations for decisions or self-correct for biases built into the design without human intervention.

Systematic issues precipitates and grows due to the monopolistic tendency of AI both in wealth and data sharing dimensions. Data wants to be shared, and to the point where all firms take the same information into account into assessing customer adequacy, certain groups can be permanently excluded from market participation. AI expands the reach of effective decision-making through the democratization of financial advice but might continue to subject segments of the population to unfair and inequitable exclusions from certain products or services. Preventative measure has to be built into the regulatory framework. It is important for regulators to support small data silos for competitive data and large data silos for collaborative and compliance data.
A further concern of AI models is post-training drift towards discrimination. As AI systems self-improve and learn, they may acquire new behaviours that have unintended consequences. Biases might either increase or decrease with circular referencing or reinforcement learning, either way with comparative regulation this can be caught before it induces permanently bad outcomes. With the light-handed regulatory approach, companies would have to prove, by clients serviced, that they understand how to detect and prevent models that discriminate against or exclude marginalized groups and individuals. Therefore, instead of auditing models, focus on auditing data and seek out to create multiple homogenous semi-centralised data platforms to allow for the automation of compliance through big data. For companies who have stepped in a bias trap, I would advise for the nominal correction of bias instead of model adaption as you will just expose yourself to other unintended consequences. Hence take advice from compliance outputs and take on customers who are the closest to but did not fully cross the acceptance threshold.

#### Implications for Companies:

Naturally more complex models are less interpretable and more accurate. Higher accuracy increases the amount of profit. The accuracy and generally the profit decreases when forced to comply with mandated biases as opposed to inherent data, model or developer biases. Machine learning entertains two types of biases. The first is a self-fulfilling bias where new lenders with new circumstances are not accepted because they don’t fit the pattern of past clients, in this case biases can persist indefinitely, even though it will be more profitable for these companies to include this new customer segment. Currently there is no good method to account for new users. The best is to test these users more theoretically or empirically using traditional models and then to slowly create a dataset of performance. A second type is mandated biases where there is a potential of profit/loss, but it is an attempt to introduce equitable solutions to minorities and the disadvantaged.

What is important to note is that, sans the persistence of inherent discrimination, these models perform very well in removing human bias as their error metrics are, to the most extent, profit-centric. Therefore, biases are financially justified. However, FIs do have the tendency to ignore long-term economic thinking when building their financial models. To this point serving the underprivileged might in fact help to create a new future financial class and lead to better economic success. Therefore, policy advisors might seek to provide guidance at the company level where deemed economically appropriate. Some of the extra profits generated from improved performance and accuracy will probably be used to subsides the disadvantaged.

Another important point for regulators to understand is that the company with the widest and largest quantity of data, ceteris paribus, would produce the least bias models. Therefore, the reduction of bias is also the result of the quantity and type of data. Therefore, one potential method of regulation is to instead of inspecting all lender models, to ask for granular lender data and to try and reproduce their results with the bias-reduction best practice.

Another option is for regulators to, instead of predicting actual biases, build up databases that include among other things the ratios between approval and denial rates, the average prices, volume and data on complaints which can be fed into a machine learning model to assess the likelihood of bias without creating hard rules. In essence regulators can use meta FIs data to create bias-prediction machines. And can then use these predictions to individually investigate the likelihood of bias and error in these firms. It is not necessary to use a machine learning approach, regulators can also settle for simple ratio analyses or scorecards.

#### Treating Biases:

The benefit of using AI systems is that they can be made to be blind to differences. The disadvantage of AI systems is that when the data is biased it might be hard to remove it from the model. For example, to remove a gender bias in a lending model, it is not as easy as easy as removing the gender variable from the dataset. The reason being that the model will find correlations of the remaining variables with the removed variables. For example, height is one of multiple variables that correlate with gender so, by simply removing gender, height would be given more importance in the model and the bias will persist.  Instead you should include as much bias relevant data as possible and then adapt the model to marginally remove these biases.

#### Data Implications:

As an example, let’s consider some implications for fair lending practices. Alternative data goes hand in hand with machine learning models and novel approaches entails novel risks. Learning from data poses risk of bias that cannot be immediately transparent. It has been shown that alternative data are the most important determinants in driving loan acceptance. This can include social media, behavioural and educational data. Anything goes, some have been noted to count the type and number of emoji’s used as well as web and transaction history. It’s a simple equation if the accuracy of the expected loss of default per additional unit of data can be improved so as to decrease default cost more than the additional cost of acquiring the data, then that data would be acquired. And in the age of the internet it has become increasingly cheap to acquire data. Within a day it is possible to create and scrape millions of social media pages at barely any costs. Lets finally have a look at the different types of biases starting with Systematic Bias and Errors (SBE) and then Progressive and Precipitous Contagion.

*1.1. SBE as an input:*
Bias present in input data, as well as incomplete or unrepresentative datasets, will limit AI’s ability to be objective.

*1.2. SBE as model:*
Similarly, choice of model might alter the proclivity to bias and errors.

1.3. *SBE as a metric.*
Choice of error metric will favour certain group behaviours over others. For example, RMSE penalises outliers more than MSE. Therefore, groups with more eradicate behaviour will be penalised.


### Progressive and Precipitous Contagion

Regulators should investigate past failures of AI and seek to ideCLntify big downside risk and disperse systematic risk. Regulators should give small data and machine silos serious thought. As AI takes an increasingly critical role in the day-to-day operations of the financial system, it poses a new source of systemic risk that has the potential to disrupt national and global economies, necessitating new controls and responses. There are immense risk of contagion where losses from negative market events may be increased by a shared algorithm passing decisions across multiple institutions. Increased risk of errors as model miscalibration would impact on many institutions simultaneously (e.g. miscalculating credit risk). This can happen progressively as these models are often self-reinforcing, so it can be a slow cycle leading to eventual catastrophe. In the same way, like algorithmically driven flash-crashes it can lead to precipitous contagion having immediate consequences.

## Adaptive AI Regulation

The regulation of AI, in particular how AI will fit into existing regulatory frameworks or require the development of new frameworks, is a topic that will continue to grow in prominence in the near future. It is expected that such progress to be gradual and for regulators to focus on developing specific standards for applying AI to particular use cases (e.g., regulating the use of AI in credit screenings, regulating the use of chat bots).

Along with the potential development for special use-cases, it is also important to create awareness and establish strategic partnerships across government agencies. In supporting the development of AI in the financial services sector, the US Treasury recommended that agencies pursue interagency efforts to advance AI and enable research and development, including engaging with the Select Committee on Artificial Intelligence, an interagency committee chaired by the White House Office of Science and Technology Policy, the National Science Foundation and the Defense Advanced Research Projects Agency, and permit real world experimentation (with appropriate limits) to better understand the benefits and risks of AI and how such technology should be appropriately regulated. I suspect that it is important to spread AI awareness throughout the state system.

Despite recognizing potential issues and challenges in fully implementing AI into the financial services industry, the US Treasury HAS indicated that the increased use of AI would provide significant benefits to the U.S. economy by “improving the quality of financial services for households and businesses and supplying a source of competitive strength for U.S. firms” and, therefore, recommended that regulators “should not impose unnecessary burdens or obstacles to the use of [AI] and should provide greater regulatory clarity that would enable further testing and responsible deployment of these technologies by regulated financial services companies as the technologies develop.

#### Comparables:

Prescriptive regulations are currently limiting the advancement of AI in financial services. The best way to track bias is with comparables, which again comes down to data collection practices. Inflexible requirements and regulators’ limited resources constrain their ability to keep up with the rapid pace of change, creating significant regulatory uncertainty for institutions seeking to use new technologies.

A big issue in AI is the tailored experience. Biases would only be identifiable using comparable data across different FIs. Then it becomes an issue of least and most biased institutions. A further issue here is the potential of collective collusion making this comparable approach useless. Multiple levels of regulations would play a role here, for example regulations governing the privacy and portability of data will shape the relative ability of financial and non-financial institutions to deploy AI and regulators to assess regulatory compliance.

It would be futile to regulate the actual systems that produce the decision making. The better alternative would be to look at the approval and denial rate by prohibited basis category. Second identifying the average price by prohibited basis category. Third it might also be worth regulators to consider application volume. Lastly, special attention should be given to increases in the level of complaints. Another potential method is to get granular data and to approach the question from the other side.

#### Automated Compliance:

One primary purpose for regulators would be to supplant otherwise costly compliance supervision with automated systems using an outcome comparison approach. It is true that regulators need to get accustomed to these new technologies but not as a means of understanding what they are regulating, rather as a means of implementing their own systems to take care of regulatory slack.
Processes such as fraud prevention and antimoney-laundering controls are currently run sub optimally. Due to data asymmetries, many activities today are run inefficiently and ineffectively − Many of these systematic inefficiencies are highly correlated; the suboptimal process at one institution has knock-on effects on other institutions and across the ecosystem − If unchecked, there is a threat of a system-wide contagion − timely response is critical to the resolution of these threats and automated compliance can help to significantly improve timely response.

#### Costly Compliance:

Increasing regulatory focus is straining institutions’ budgets. Cumulative financial penalties for non-compliance between 2009 and 2017 totalled $342 billion globally, with 89% of industry executives around the world expecting continued increases in compliance costs from 2017–2019. Regulatory priorities stretch beyond leverage and capital-adequacy requirements, and there is also increased focus on collective issues such as financial crime, privacy and data security. New regulatory requirements that emerge to address these areas will place pressure on operating budgets for institutions.

Inarguably, compliance management related costs for institutions have been rising significantly. As per the Thomson Reuters 9th annual Cost of Compliance survey (conducted in Q1 2018) of compliance & risk practitioners from around 800 FIs across the globe, 66% of FIs expect the cost of senior compliance staff to increase[[18]](#). According to LexisNexis Risk Solutions’ “Future Financial Crime Risks 2017” survey of over 170 senior financial crime compliance management pros from banks in UK, 63% of the survey respondents saw a rise in financial crime compliance management costs over the past two years[[19]](#).

One thing is sure in the age of machines, traditional regulatory methods is going to become excruciatingly hard if not downright impossible. Costs on the side of both enforcement and compliance are bound to keep on increasing. Both sides of the regulatory equation have to deeply understand their models to ensure fair use as cost pressures might lead to institutions and regulators skipping this step.

#### AI as Company:

AI implemented in a company is nothing more than a function replicator or automaton, to this extent whatever the corporate motive is, the AI motive becomes. Thus, we need to focus at the corporate level to ensure that AI enables a fairer, stable and more inclusive financial system as it risks doing the opposite without supervision. These regulations should be technology neutral, in that the technology should not be regulated, but much rather the activities and the behaviours that result from model predictions. Thus, model output data and business activity data would become of crucial importance.

#### Sandbox:

AI Technology is too revolutionary to simply rely on past regulatory actions after supposed ‘paradigm moving’ technologies. The age of AI calls for scientific reactive regulations rather than costly preventative measures. And adaptive sandbox strategies might just be the correct solution. There is a certain beauty in small isolated failures. Regulation should not hold back the failure of AI. AI is currently isolated and embedded in single institutions, before wide implementation we should allow for failures to first learn how to deal with isolated uncertainties. Regulations need not be rewritten, but implementation simply sandboxed with customer protection guarantees.

#### Broadly Written:

Regulations should be more broadly written. Regulations will simply not be followed in the new do-first-apologise-later era. The aggregate effect of narrow regulations would simply be to constrain innovation and national competitiveness. It is important to create space for strategic national competition to ensure lower barriers to entry and the easier adoption and integration of technologies. The digital nature of the future has led to a playing field of easily identifiable and rectifiable strategies with minimal cost implications. We have entered the golden age of experiment and experiment we should. For some reason the complexity surrounding AI, combined with regulators’ natural tendency to be cautious, suggests that regulatory frameworks are likely to lean in favour of risk mitigation. This would be an absurd outcome, as automation overall should lead to less regulation qualitatively. Regulators should not see AI as complex, it should simply see it as a much better way to make predictions and act on decisions. More automation almost always leads to better aggregate accuracy, so special attention should rather be given to systematic inaccuracies.

#### Cloud and Cybersecurity:

Increased interconnectivity as AI demands increased digitisation and linkages across institutions, creates more opportunities for cyberintrusion. The ability to use cloud-based services, the ability to use public and private cloud infrastructure, and to use the kinds of data that can be hosted on the cloud, is critical to the development of AI applications. Regulations on cloud usage by financial institutions vary globally, with stricter restrictions in Europe. Technology players in regions with more relaxed rules have an advantage in developing new capabilities. It is essential for cloud technologies to be trusted and audited. It is a technology that is not too hard to audit. Cloud technologies should be allowed, but with supervision.

Growing cyber-risks present increasing operational challenges Institutions must develop strategies to mitigate the increasing risk of abuse and leakage of highly confidential information at a customer and transaction level, as well as the increased risk-sharing of sensitive competitive information. Here penalties are not necessary, reputational damage more than enough takes care of this issue. Instead stealthily sharing of data among competitors should not be permissible as it will lead to data oligopolies. This is a form of price collusion, however, now it extends much further than just the price.

#### Real-time analysis:

Many have speculated that real-time payment brings with it “real-time fraud”. There is evidence to support this contention: the UK experienced a 132% increase in fraud in the year after it implemented the Faster Payments scheme and, thanks to automation, insurance fraud cases are expected to double. Furthermore, experts have warned that the proliferation of AI technologies could enable new forms of cybercrime and other threats across different industries.

#### Regional Frameworks:

Differences in data standards and regulation are apparent across regions. This challenges institutions that operate globally and necessitates international frameworks to manage common systemic issues that would have an impact across jurisdictions. Risks such as rogue trading, capital adequacy and cybersecurity require cross-border collaboration to reach a resolution. These companies have to expand and grow to a point where they can attempt interesting experiments with enough capital. These regulations need not be world-wide even small regions can agree to share frameworks. The rest would adopt good policies voluntarily. Further it is good to have individual countries test out different regulations. Country wide implementation can in itself serve as a pseudo-sandbox for other partnering countries. Countries can decide amongst themselves to test out various experiments in order to scientifically verify policies before they extend to the greater region.

The increasing breadth and depth of data flows within and across organizations (e.g. through screen scrapers) increases the risk of improper use and data breaches, potentially leading to an increased need for regulatory oversight in data management and sharing. Global data regulations are in fact undergoing a period of unprecedented change as governments move to adopt new rules to protect and empower citizens. However, regulators have to also know that it is often easy to effectively translocate AI models developed in more permissive data regimes to less permissive ones, and this should be given great thought when considering competitive dynamics. For example, Google does not need all the data from EU citizens to drive certain campaigns, they just need a few bread crumbs to expand models that are trained by proxy countries like Canada, Australasia and the US.

The revised Payment Services Directive (PSD2) of the European Union came into force in January 2018, with the aim of enabling more innovative payments across Europe. In conjunction with the General Data Protection Regulation (GDPR), this means institutions have to carefully balance requirements to share data with third parties against the risk of substantial penalties in cases where data is mishandled. International regulatory frameworks have the disadvantage of allowing competing countries strip your country of their moat. Hence to the point where money becomes very mobile as well as international investment or deposit practices, then larger monopolies would to a great extent be to the benefit of the country. However, we are still some way away from that.
Data regulations formulated in the coming years will have long lasting effects on financial markets. In many jurisdictions, data regulations are still being developed. In the coming years, these regulations will solidify, and financial markets will be shaped by those regulations for the foreseeable future.

What norms will develop regarding international data flows, and how will divergent domestic rules affect cross border data flows?[[20]](#)


## Future Analysis

In the future it might be worth investigating different business models that includes the use of AI systems under the following regulatory classifications.


- Prudential regulations -- created for the purpose of addressing excessive risk taking. This can be further classified into micro and macro prudential regulation. Micro is to address issues relating to a single institution and macro to the whole market.
  - Get a log output of customers default rates.
- Market structure regulation -- addresses issues that is embedded into markets like asymmetric information.
  -  Ensure that customers fairly report their financial position.
- Conduct regulation -- seeks to protect customers from not being able to properly assess the risks and rewards of financial products.
  - Ensure that both borrowers and lenders are aware of the financial consequences of defaults and interest payments. 
- Public interest regulation -- are in place to guard against the use of the financial system for illicit purposes like money laundering.
  - Make sure that lenders are not receiving money from known terrorists or money launders.




 
I want to thank [Apolline Blandin](https://www.jbs.cam.ac.uk/faculty-research/centres/alternative-finance/people/) and [Leon Szeli](https://web.stanford.edu/group/design_education/cgi-bin/mediawiki/index.php/Leon_Szeli) for their contribution.
 
 


----------
- [[1]](#) https://www.fca.org.uk/news/speeches/ai-and-financial-crime-silver-bullet-or-red-herring
- [[2]](#) https://financefeeds.com/bank-russia-embraces-machine-learning-heart-new-economic-indicator/
- [[3]](#) https://www.bloomberg.com/news/articles/2017-12-18/central-banks-are-turning-to-big-data-to-help-them-craft-policy
- [[4]](#) https://www.marketsmedia.com/finra-taps-ai-to-stop-mini-manipulation/
- [[5]](#) https://www.sec.gov/news/speech/bauguess-big-data-ai
- [[6]](#)https://www.arc.gov.au/news-publications/publications/making-difference-outcomes-arc-supported-research-2016-17/new-software-detect-money-laundering
- [[7]](#)https://download.asic.gov.au/media/4064271/greg-medcraft-speech-corp-governance-discussion-group-published-3-november-2016.pdf
- [[8]](#) http://www.finra.org/industry/special-notice-073018
- [[9]](#) https://www.christies.com/features/A-collaboration-between-two-artists-one-human-one-a-machine-9332-1.aspx
- [[10]](#) https://www.wired.com/story/researcher-fooled-a-google-ai-into-thinking-a-rifle-was-a-helicopter/
- [[11]](#) https://arxiv.org/abs/1609.02943
- [[12]](#) https://www.kidspot.com.au/birth/pregnancy/miscarriage/i-had-a-miscarriage-and-social-media-hurt-me-in-the-strangest-way/news-story/beaeb4be46b290dbb1e2aacae4caaa1d
- [[13]](#) https://www.theguardian.com/news/series/cambridge-analytica-files
- [[14]](#) http://www3.weforum.org/docs/WEF_New_Physics_of_Financial_Services.pdf
- [[15]](#) https://arxiv.org/pdf/1801.05534
- [[16]](#) http://www3.weforum.org/docs/WEF_New_Physics_of_Financial_Services.pdf
- [[17]](#) https://www.realme.govt.nz/
- [[18]](#) https://risk.thomsonreuters.com/en/resources/special-report/cost-compliance-2018.html
- [[19]](#) https://risk.lexisnexis.co.uk/insights-resources/white-paper/future-financial-crime-risks-2017-wp-uk
- [[20]](#) http://www3.weforum.org/docs/WEF_New_Physics_of_Financial_Services.pdf

